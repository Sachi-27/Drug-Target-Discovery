{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data we have:\n",
    "1. Protein sequences in string format\n",
    "2. Protein PDB sequences in string format<br>\n",
    "\n",
    "We need to use these to predict druggability of the protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of druggable proteins: 3345\n",
      "Number of approved druggable proteins: 2652\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "with open(\"../drugbank/protein_drugbank.json\") as f:\n",
    "    drugbank_info = json.load(f)\n",
    "    druggable_proteins = list(drugbank_info.keys())\n",
    "\n",
    "print(\"Number of druggable proteins:\", len(druggable_proteins))\n",
    "# encoding protein\n",
    "approved_druggable_proteins = []\n",
    "for protein in druggable_proteins:\n",
    "    for k,v in drugbank_info[protein].items():\n",
    "        if \"approved\" in v[1]:\n",
    "            approved_druggable_proteins.append(protein)\n",
    "            break\n",
    "\n",
    "print(\"Number of approved druggable proteins:\", len(approved_druggable_proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of proteins: 20434\n"
     ]
    }
   ],
   "source": [
    "with open(\"pdb_sequences.json\") as f:\n",
    "    pdb_sequences = json.load(f)\n",
    "\n",
    "proteins = list(pdb_sequences.keys())\n",
    "print(\"Total number of proteins:\", len(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins with pdb_seq: 7347\n"
     ]
    }
   ],
   "source": [
    "# From the pdb_sequences, find number of proteins, which have pdb_seq info\n",
    "pdb_seq_proteins = []\n",
    "for protein in proteins:\n",
    "    if pdb_sequences[protein][\"pdb_seq\"] != \"N\"*len(pdb_sequences[protein][\"pdb_seq\"]):\n",
    "        pdb_seq_proteins.append(protein)\n",
    "\n",
    "print(\"Number of proteins with pdb_seq:\", len(pdb_seq_proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of druggable proteins with pdb_seq: 1755\n"
     ]
    }
   ],
   "source": [
    "# Find number of druggable and non druggable proteins in this pdb_seq_proteins\n",
    "# We consider only those with approved drugs as druggable, rest all are non druggable\n",
    "druggable_pdb_seq_proteins = []\n",
    "for protein in pdb_seq_proteins:\n",
    "    if protein in approved_druggable_proteins:\n",
    "        druggable_pdb_seq_proteins.append(protein)\n",
    "\n",
    "print(\"Number of druggable proteins with pdb_seq:\", len(druggable_pdb_seq_proteins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an initial attempt, we choose features as\n",
    "1. Amino Acid Composition\n",
    "2. Beta-Helix-Turn Composition\n",
    "\n",
    "Ablation Study<br>\n",
    "a. Only Property 1<br>\n",
    "b. Property 1 + Property 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'M': 97991,\n",
       "  'G': 293339,\n",
       "  'A': 309713,\n",
       "  'T': 240378,\n",
       "  'Q': 213298,\n",
       "  'L': 430478,\n",
       "  'V': 273214,\n",
       "  'F': 159932,\n",
       "  'E': 326578,\n",
       "  'K': 267489,\n",
       "  'I': 200176,\n",
       "  'R': 249418,\n",
       "  'H': 110857,\n",
       "  'S': 363990,\n",
       "  'N': 169177,\n",
       "  'P': 278523,\n",
       "  'Y': 121627,\n",
       "  'C': 92974,\n",
       "  'D': 227557,\n",
       "  'W': 52558},\n",
       " 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique characters in sequnces of proteins+ their frequency\n",
    "unique_chars = {}\n",
    "for protein in pdb_seq_proteins:\n",
    "    for char in pdb_sequences[protein][\"seq\"]:\n",
    "        if char in unique_chars:\n",
    "            unique_chars[char] += 1\n",
    "        else:\n",
    "            unique_chars[char] = 1\n",
    "unique_chars, len(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amino_acid_composition(seq):\n",
    "    amino_acids = unique_chars.keys()\n",
    "    composition = {}\n",
    "    for aa in amino_acids:\n",
    "        composition[aa] = seq.count(aa)/len(seq)\n",
    "    return list(composition.values())\n",
    "\n",
    "def get_pdb_composition(pdb_seq):\n",
    "    pdb_components = [\"N\", \"H\", \"E\", \"T\"]\n",
    "    composition = {}\n",
    "    for aa in pdb_components:\n",
    "        composition[aa] = pdb_seq.count(aa)/len(pdb_seq)\n",
    "    return list(composition.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amino Acid Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect features X and labels Y\n",
    "# X being amino acid composition of protein sequence and Y being druggable(1) or not(0)\n",
    "X = []\n",
    "Y = []\n",
    "for protein in pdb_seq_proteins:\n",
    "    X.append(get_amino_acid_composition(pdb_sequences[protein][\"seq\"]))\n",
    "    if protein in druggable_pdb_seq_proteins:\n",
    "        Y.append(1)\n",
    "    else:\n",
    "        Y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7347, 20), (7347,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1755, 20), (5592, 20))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate druggable and non druggable X\n",
    "X_druggable = X[Y==1]\n",
    "X_non_druggable = X[Y==0]\n",
    "X_druggable.shape, X_non_druggable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 20), (2000,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's choose 1000 druggable and 1000 non druggable proteins for training\n",
    "# Shuffle the data using np.random.shuffle\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(X_druggable)\n",
    "np.random.shuffle(X_non_druggable)\n",
    "\n",
    "X_train = np.concatenate((X_druggable[:1000], X_non_druggable[:1000]), axis=0)\n",
    "Y_train = np.concatenate((np.ones(1000), np.zeros(1000)), axis=0)\n",
    "\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 20), (2000,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate X_train and Y_train\n",
    "data = np.concatenate((X_train, Y_train.reshape(-1,1)), axis=1)\n",
    "data.shape\n",
    "\n",
    "# random shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Break the data back into X_train and Y_train\n",
    "X_train = data[:, :-1]\n",
    "Y_train = data[:, -1]\n",
    "\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(l1_ratio=0.5, max_iter=1000, penalty=&#x27;elasticnet&#x27;,\n",
       "                   solver=&#x27;saga&#x27;, warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(l1_ratio=0.5, max_iter=1000, penalty=&#x27;elasticnet&#x27;,\n",
       "                   solver=&#x27;saga&#x27;, warm_start=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(l1_ratio=0.5, max_iter=1000, penalty='elasticnet',\n",
       "                   solver='saga', warm_start=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(penalty=\"elasticnet\", warm_start=True, solver=\"saga\", l1_ratio=0.5, max_iter=1000)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(model, X, Y):\n",
    "    Y_pred = model.predict(X)\n",
    "    TP = np.sum((Y==1) & (Y_pred==1))\n",
    "    FN = np.sum((Y==1) & (Y_pred==0))\n",
    "    FP = np.sum((Y==0) & (Y_pred==1))\n",
    "    TN = np.sum((Y==0) & (Y_pred==0))\n",
    "\n",
    "    print(\"TP:\", TP, \"FN:\", FN, \"FP:\", FP, \"TN:\", TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 399, 601, 309)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give training stats on the model\n",
    "# TP, TN, FP, FN\n",
    "Y_pred = model.predict(X_train)\n",
    "TP = np.sum((Y_train==1) & (Y_pred==1))\n",
    "FP = np.sum((Y_train==0) & (Y_pred==1))\n",
    "TN = np.sum((Y_train==0) & (Y_pred==0))\n",
    "FN = np.sum((Y_train==1) & (Y_pred==0))\n",
    "TP, FP, TN, FN # Not able to predict any druggable protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(523, 1801, 2791, 232)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.concatenate((X_druggable[1000:], X_non_druggable[1000:]), axis=0)\n",
    "Y_test = np.concatenate((np.ones(len(X_druggable[1000:])), np.zeros(len(X_non_druggable[1000:]))), axis=0)\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "TP = np.sum((Y_test==1) & (Y_pred==1))\n",
    "FP = np.sum((Y_test==0) & (Y_pred==1))\n",
    "TN = np.sum((Y_test==0) & (Y_pred==0))\n",
    "FN = np.sum((Y_test==1) & (Y_pred==0))\n",
    "TP, FP, TN, FN # Not able to predict any druggable protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69380108\n",
      "Iteration 2, loss = 0.69281519\n",
      "Iteration 3, loss = 0.69233447\n",
      "Iteration 4, loss = 0.69166630\n",
      "Iteration 5, loss = 0.69071366\n",
      "Iteration 6, loss = 0.68997465\n",
      "Iteration 7, loss = 0.68851109\n",
      "Iteration 8, loss = 0.68628043\n",
      "Iteration 9, loss = 0.68329588\n",
      "Iteration 10, loss = 0.68004182\n",
      "Iteration 11, loss = 0.67474920\n",
      "Iteration 12, loss = 0.66873077\n",
      "Iteration 13, loss = 0.66164489\n",
      "Iteration 14, loss = 0.65218190\n",
      "Iteration 15, loss = 0.64387998\n",
      "Iteration 16, loss = 0.63931488\n",
      "Iteration 17, loss = 0.63534236\n",
      "Iteration 18, loss = 0.63147927\n",
      "Iteration 19, loss = 0.62911596\n",
      "Iteration 20, loss = 0.62936106\n",
      "Iteration 21, loss = 0.62775759\n",
      "Iteration 22, loss = 0.63137808\n",
      "Iteration 23, loss = 0.62688726\n",
      "Iteration 24, loss = 0.62990913\n",
      "Iteration 25, loss = 0.62835263\n",
      "Iteration 26, loss = 0.63015376\n",
      "Iteration 27, loss = 0.62690945\n",
      "Iteration 28, loss = 0.62606203\n",
      "Iteration 29, loss = 0.62634701\n",
      "Iteration 30, loss = 0.62496641\n",
      "Iteration 31, loss = 0.62665972\n",
      "Iteration 32, loss = 0.62644231\n",
      "Iteration 33, loss = 0.62555583\n",
      "Iteration 34, loss = 0.62451037\n",
      "Iteration 35, loss = 0.62873527\n",
      "Iteration 36, loss = 0.62887558\n",
      "Iteration 37, loss = 0.62727037\n",
      "Iteration 38, loss = 0.62767833\n",
      "Iteration 39, loss = 0.62345903\n",
      "Iteration 40, loss = 0.62430230\n",
      "Iteration 41, loss = 0.62274813\n",
      "Iteration 42, loss = 0.62219456\n",
      "Iteration 43, loss = 0.62452094\n",
      "Iteration 44, loss = 0.62420353\n",
      "Iteration 45, loss = 0.62511149\n",
      "Iteration 46, loss = 0.62778897\n",
      "Iteration 47, loss = 0.62550064\n",
      "Iteration 48, loss = 0.62169482\n",
      "Iteration 49, loss = 0.62204819\n",
      "Iteration 50, loss = 0.62395315\n",
      "Iteration 51, loss = 0.62633181\n",
      "Iteration 52, loss = 0.62225644\n",
      "Iteration 53, loss = 0.62069679\n",
      "Iteration 54, loss = 0.62112414\n",
      "Iteration 55, loss = 0.63134571\n",
      "Iteration 56, loss = 0.62146528\n",
      "Iteration 57, loss = 0.62146741\n",
      "Iteration 58, loss = 0.62099009\n",
      "Iteration 59, loss = 0.62223472\n",
      "Iteration 60, loss = 0.62001988\n",
      "Iteration 61, loss = 0.62051025\n",
      "Iteration 62, loss = 0.61926272\n",
      "Iteration 63, loss = 0.61855584\n",
      "Iteration 64, loss = 0.61817691\n",
      "Iteration 65, loss = 0.61900737\n",
      "Iteration 66, loss = 0.62090653\n",
      "Iteration 67, loss = 0.61929222\n",
      "Iteration 68, loss = 0.62235639\n",
      "Iteration 69, loss = 0.61884784\n",
      "Iteration 70, loss = 0.61715990\n",
      "Iteration 71, loss = 0.61725666\n",
      "Iteration 72, loss = 0.61899655\n",
      "Iteration 73, loss = 0.61715185\n",
      "Iteration 74, loss = 0.61751170\n",
      "Iteration 75, loss = 0.61605519\n",
      "Iteration 76, loss = 0.61775353\n",
      "Iteration 77, loss = 0.61663225\n",
      "Iteration 78, loss = 0.61712370\n",
      "Iteration 79, loss = 0.61507622\n",
      "Iteration 80, loss = 0.61472622\n",
      "Iteration 81, loss = 0.61579460\n",
      "Iteration 82, loss = 0.61512019\n",
      "Iteration 83, loss = 0.61488028\n",
      "Iteration 84, loss = 0.61515407\n",
      "Iteration 85, loss = 0.61680596\n",
      "Iteration 86, loss = 0.61902182\n",
      "Iteration 87, loss = 0.61517870\n",
      "Iteration 88, loss = 0.61364150\n",
      "Iteration 89, loss = 0.61773247\n",
      "Iteration 90, loss = 0.61224050\n",
      "Iteration 91, loss = 0.61266191\n",
      "Iteration 92, loss = 0.61191299\n",
      "Iteration 93, loss = 0.61026968\n",
      "Iteration 94, loss = 0.61358108\n",
      "Iteration 95, loss = 0.61129822\n",
      "Iteration 96, loss = 0.61335237\n",
      "Iteration 97, loss = 0.61379300\n",
      "Iteration 98, loss = 0.61008752\n",
      "Iteration 99, loss = 0.60881107\n",
      "Iteration 100, loss = 0.60799730\n",
      "Iteration 101, loss = 0.62148604\n",
      "Iteration 102, loss = 0.61964072\n",
      "Iteration 103, loss = 0.61622278\n",
      "Iteration 104, loss = 0.61176007\n",
      "Iteration 105, loss = 0.60917627\n",
      "Iteration 106, loss = 0.60928065\n",
      "Iteration 107, loss = 0.60683639\n",
      "Iteration 108, loss = 0.60558901\n",
      "Iteration 109, loss = 0.60668045\n",
      "Iteration 110, loss = 0.60486444\n",
      "Iteration 111, loss = 0.60430952\n",
      "Iteration 112, loss = 0.60404699\n",
      "Iteration 113, loss = 0.60577027\n",
      "Iteration 114, loss = 0.60532343\n",
      "Iteration 115, loss = 0.60314991\n",
      "Iteration 116, loss = 0.60595522\n",
      "Iteration 117, loss = 0.60432979\n",
      "Iteration 118, loss = 0.60483599\n",
      "Iteration 119, loss = 0.60142993\n",
      "Iteration 120, loss = 0.60339947\n",
      "Iteration 121, loss = 0.60032089\n",
      "Iteration 122, loss = 0.60289704\n",
      "Iteration 123, loss = 0.60282332\n",
      "Iteration 124, loss = 0.59915972\n",
      "Iteration 125, loss = 0.59856749\n",
      "Iteration 126, loss = 0.60098628\n",
      "Iteration 127, loss = 0.60099077\n",
      "Iteration 128, loss = 0.59764393\n",
      "Iteration 129, loss = 0.59453163\n",
      "Iteration 130, loss = 0.59765519\n",
      "Iteration 131, loss = 0.59826040\n",
      "Iteration 132, loss = 0.59531162\n",
      "Iteration 133, loss = 0.59647179\n",
      "Iteration 134, loss = 0.59209700\n",
      "Iteration 135, loss = 0.59162849\n",
      "Iteration 136, loss = 0.59145365\n",
      "Iteration 137, loss = 0.59179762\n",
      "Iteration 138, loss = 0.58860940\n",
      "Iteration 139, loss = 0.58861281\n",
      "Iteration 140, loss = 0.58636203\n",
      "Iteration 141, loss = 0.58722807\n",
      "Iteration 142, loss = 0.58917000\n",
      "Iteration 143, loss = 0.58562204\n",
      "Iteration 144, loss = 0.58885026\n",
      "Iteration 145, loss = 0.58406731\n",
      "Iteration 146, loss = 0.59069349\n",
      "Iteration 147, loss = 0.58865703\n",
      "Iteration 148, loss = 0.58571978\n",
      "Iteration 149, loss = 0.58680901\n",
      "Iteration 150, loss = 0.58313326\n",
      "Iteration 151, loss = 0.58091050\n",
      "Iteration 152, loss = 0.57878971\n",
      "Iteration 153, loss = 0.58113961\n",
      "Iteration 154, loss = 0.58108244\n",
      "Iteration 155, loss = 0.57951379\n",
      "Iteration 156, loss = 0.57830123\n",
      "Iteration 157, loss = 0.57729755\n",
      "Iteration 158, loss = 0.57665224\n",
      "Iteration 159, loss = 0.57978712\n",
      "Iteration 160, loss = 0.57566216\n",
      "Iteration 161, loss = 0.57595018\n",
      "Iteration 162, loss = 0.58509837\n",
      "Iteration 163, loss = 0.57946952\n",
      "Iteration 164, loss = 0.59159338\n",
      "Iteration 165, loss = 0.59123255\n",
      "Iteration 166, loss = 0.57668536\n",
      "Iteration 167, loss = 0.57221764\n",
      "Iteration 168, loss = 0.57027849\n",
      "Iteration 169, loss = 0.57335826\n",
      "Iteration 170, loss = 0.57074000\n",
      "Iteration 171, loss = 0.57022435\n",
      "Iteration 172, loss = 0.57409483\n",
      "Iteration 173, loss = 0.58255732\n",
      "Iteration 174, loss = 0.58486406\n",
      "Iteration 175, loss = 0.57432905\n",
      "Iteration 176, loss = 0.56734972\n",
      "Iteration 177, loss = 0.56578115\n",
      "Iteration 178, loss = 0.56747001\n",
      "Iteration 179, loss = 0.56707469\n",
      "Iteration 180, loss = 0.56406049\n",
      "Iteration 181, loss = 0.56943181\n",
      "Iteration 182, loss = 0.56895383\n",
      "Iteration 183, loss = 0.56670734\n",
      "Iteration 184, loss = 0.56232113\n",
      "Iteration 185, loss = 0.56982335\n",
      "Iteration 186, loss = 0.57371968\n",
      "Iteration 187, loss = 0.56948381\n",
      "Iteration 188, loss = 0.56327231\n",
      "Iteration 189, loss = 0.56043549\n",
      "Iteration 190, loss = 0.56000281\n",
      "Iteration 191, loss = 0.56118064\n",
      "Iteration 192, loss = 0.56062821\n",
      "Iteration 193, loss = 0.56043221\n",
      "Iteration 194, loss = 0.55918616\n",
      "Iteration 195, loss = 0.56047863\n",
      "Iteration 196, loss = 0.55873578\n",
      "Iteration 197, loss = 0.56563529\n",
      "Iteration 198, loss = 0.57390723\n",
      "Iteration 199, loss = 0.55907768\n",
      "Iteration 200, loss = 0.55725848\n",
      "Iteration 201, loss = 0.55627873\n",
      "Iteration 202, loss = 0.55322048\n",
      "Iteration 203, loss = 0.55492272\n",
      "Iteration 204, loss = 0.55141939\n",
      "Iteration 205, loss = 0.55984562\n",
      "Iteration 206, loss = 0.55224001\n",
      "Iteration 207, loss = 0.55209353\n",
      "Iteration 208, loss = 0.55353474\n",
      "Iteration 209, loss = 0.55141569\n",
      "Iteration 210, loss = 0.55621836\n",
      "Iteration 211, loss = 0.57172732\n",
      "Iteration 212, loss = 0.56530515\n",
      "Iteration 213, loss = 0.54978610\n",
      "Iteration 214, loss = 0.54935417\n",
      "Iteration 215, loss = 0.55719735\n",
      "Iteration 216, loss = 0.55328158\n",
      "Iteration 217, loss = 0.55259816\n",
      "Iteration 218, loss = 0.54576370\n",
      "Iteration 219, loss = 0.54568157\n",
      "Iteration 220, loss = 0.55327447\n",
      "Iteration 221, loss = 0.55632069\n",
      "Iteration 222, loss = 0.54663227\n",
      "Iteration 223, loss = 0.55601229\n",
      "Iteration 224, loss = 0.54848013\n",
      "Iteration 225, loss = 0.54516547\n",
      "Iteration 226, loss = 0.54564162\n",
      "Iteration 227, loss = 0.54315997\n",
      "Iteration 228, loss = 0.54609916\n",
      "Iteration 229, loss = 0.54204203\n",
      "Iteration 230, loss = 0.54420227\n",
      "Iteration 231, loss = 0.54661557\n",
      "Iteration 232, loss = 0.54725310\n",
      "Iteration 233, loss = 0.54410641\n",
      "Iteration 234, loss = 0.53933706\n",
      "Iteration 235, loss = 0.54364359\n",
      "Iteration 236, loss = 0.54029701\n",
      "Iteration 237, loss = 0.54119402\n",
      "Iteration 238, loss = 0.54337846\n",
      "Iteration 239, loss = 0.53916477\n",
      "Iteration 240, loss = 0.54228783\n",
      "Iteration 241, loss = 0.54151033\n",
      "Iteration 242, loss = 0.54081944\n",
      "Iteration 243, loss = 0.53809949\n",
      "Iteration 244, loss = 0.54444535\n",
      "Iteration 245, loss = 0.53970035\n",
      "Iteration 246, loss = 0.53860582\n",
      "Iteration 247, loss = 0.53661189\n",
      "Iteration 248, loss = 0.54352446\n",
      "Iteration 249, loss = 0.53957198\n",
      "Iteration 250, loss = 0.54761392\n",
      "Iteration 251, loss = 0.54014124\n",
      "Iteration 252, loss = 0.54298711\n",
      "Iteration 253, loss = 0.54512138\n",
      "Iteration 254, loss = 0.54382631\n",
      "Iteration 255, loss = 0.54193193\n",
      "Iteration 256, loss = 0.54380642\n",
      "Iteration 257, loss = 0.54459302\n",
      "Iteration 258, loss = 0.53616811\n",
      "Iteration 259, loss = 0.53379141\n",
      "Iteration 260, loss = 0.53949247\n",
      "Iteration 261, loss = 0.53621872\n",
      "Iteration 262, loss = 0.53114899\n",
      "Iteration 263, loss = 0.53200111\n",
      "Iteration 264, loss = 0.53867332\n",
      "Iteration 265, loss = 0.53173193\n",
      "Iteration 266, loss = 0.54535305\n",
      "Iteration 267, loss = 0.54020844\n",
      "Iteration 268, loss = 0.54284240\n",
      "Iteration 269, loss = 0.53084031\n",
      "Iteration 270, loss = 0.52943451\n",
      "Iteration 271, loss = 0.52920125\n",
      "Iteration 272, loss = 0.52652337\n",
      "Iteration 273, loss = 0.52440300\n",
      "Iteration 274, loss = 0.52551715\n",
      "Iteration 275, loss = 0.53600029\n",
      "Iteration 276, loss = 0.54462501\n",
      "Iteration 277, loss = 0.53190463\n",
      "Iteration 278, loss = 0.53071410\n",
      "Iteration 279, loss = 0.52711012\n",
      "Iteration 280, loss = 0.52440434\n",
      "Iteration 281, loss = 0.53361767\n",
      "Iteration 282, loss = 0.52708736\n",
      "Iteration 283, loss = 0.53135659\n",
      "Iteration 284, loss = 0.53135629\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-24 {color: black;}#sk-container-id-24 pre{padding: 0;}#sk-container-id-24 div.sk-toggleable {background-color: white;}#sk-container-id-24 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-24 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-24 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-24 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-24 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-24 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-24 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-24 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-24 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-24 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-24 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-24 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-24 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-24 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-24 div.sk-item {position: relative;z-index: 1;}#sk-container-id-24 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-24 div.sk-item::before, #sk-container-id-24 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-24 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-24 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-24 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-24 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-24 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-24 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-24 div.sk-label-container {text-align: center;}#sk-container-id-24 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-24 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-24\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(100, 50, 50, 10), learning_rate=&#x27;adaptive&#x27;,\n",
       "              max_iter=1000, verbose=True, warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" checked><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(100, 50, 50, 10), learning_rate=&#x27;adaptive&#x27;,\n",
       "              max_iter=1000, verbose=True, warm_start=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 50, 50, 10), learning_rate='adaptive',\n",
       "              max_iter=1000, verbose=True, warm_start=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use a neural network model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "np.random.seed(0)\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 50, 50, 10), max_iter=1000, warm_start=True, solver=\"adam\", activation=\"relu\", learning_rate=\"adaptive\", verbose=True)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 769 FN: 231 FP: 331 TN: 669\n"
     ]
    }
   ],
   "source": [
    "print_stats(model, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 526 FN: 229 FP: 1874 TN: 2718\n"
     ]
    }
   ],
   "source": [
    "print_stats(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.769 0.669\n",
      "0.6966887417218544 0.5918989547038328\n"
     ]
    }
   ],
   "source": [
    "print(769/1000, 669/1000) # Training accuracy in positive and negative classes\n",
    "print(526/(526+229), 2718/(1874+2718)) # Testing accuracy in positive and negative classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amino Acid + PDB Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7347, 24), (7347,))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for protein in pdb_seq_proteins:\n",
    "    X.append(get_amino_acid_composition(pdb_sequences[protein][\"seq\"]) + get_pdb_composition(pdb_sequences[protein][\"pdb_seq\"]))\n",
    "    if protein in druggable_pdb_seq_proteins:\n",
    "        Y.append(1)\n",
    "    else:\n",
    "        Y.append(0)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 24), (2000,), (5347, 24), (5347,))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_druggable = X[Y==1]\n",
    "X_non_druggable = X[Y==0]\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(X_druggable)\n",
    "np.random.shuffle(X_non_druggable)\n",
    "\n",
    "X_train = np.concatenate((X_druggable[:1000], X_non_druggable[:1000]), axis=0)\n",
    "Y_train = np.concatenate((np.ones(1000), np.zeros(1000)), axis=0)\n",
    "\n",
    "data = np.concatenate((X_train, Y_train.reshape(-1,1)), axis=1)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X_train = data[:, :-1]\n",
    "Y_train = data[:, -1]\n",
    "\n",
    "X_test = np.concatenate((X_druggable[1000:], X_non_druggable[1000:]), axis=0)\n",
    "Y_test = np.concatenate((np.ones(len(X_druggable[1000:])), np.zeros(len(X_non_druggable[1000:]))), axis=0)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 676 FN: 324 FP: 423 TN: 577\n",
      "TP: 526 FN: 229 FP: 1872 TN: 2720\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression model\n",
    "np.random.seed(0)\n",
    "model = LogisticRegression(penalty=\"elasticnet\", warm_start=True, solver=\"saga\", l1_ratio=0.5, max_iter=1000)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print_stats(model, X_train, Y_train)\n",
    "print_stats(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71986156\n",
      "Iteration 2, loss = 0.68871544\n",
      "Iteration 3, loss = 0.68301885\n",
      "Iteration 4, loss = 0.67471392\n",
      "Iteration 5, loss = 0.66269385\n",
      "Iteration 6, loss = 0.65569090\n",
      "Iteration 7, loss = 0.65155381\n",
      "Iteration 8, loss = 0.65076291\n",
      "Iteration 9, loss = 0.64876795\n",
      "Iteration 10, loss = 0.64774108\n",
      "Iteration 11, loss = 0.64635131\n",
      "Iteration 12, loss = 0.64460503\n",
      "Iteration 13, loss = 0.64381048\n",
      "Iteration 14, loss = 0.64451909\n",
      "Iteration 15, loss = 0.64324714\n",
      "Iteration 16, loss = 0.63942918\n",
      "Iteration 17, loss = 0.64129227\n",
      "Iteration 18, loss = 0.63827998\n",
      "Iteration 19, loss = 0.63748101\n",
      "Iteration 20, loss = 0.63630674\n",
      "Iteration 21, loss = 0.63414008\n",
      "Iteration 22, loss = 0.63139745\n",
      "Iteration 23, loss = 0.63114305\n",
      "Iteration 24, loss = 0.62740440\n",
      "Iteration 25, loss = 0.62601836\n",
      "Iteration 26, loss = 0.62556172\n",
      "Iteration 27, loss = 0.62191620\n",
      "Iteration 28, loss = 0.62192704\n",
      "Iteration 29, loss = 0.61953621\n",
      "Iteration 30, loss = 0.61863272\n",
      "Iteration 31, loss = 0.62181570\n",
      "Iteration 32, loss = 0.62097137\n",
      "Iteration 33, loss = 0.61944067\n",
      "Iteration 34, loss = 0.61453308\n",
      "Iteration 35, loss = 0.61228663\n",
      "Iteration 36, loss = 0.61887594\n",
      "Iteration 37, loss = 0.61259384\n",
      "Iteration 38, loss = 0.61128188\n",
      "Iteration 39, loss = 0.61239039\n",
      "Iteration 40, loss = 0.60868513\n",
      "Iteration 41, loss = 0.60690491\n",
      "Iteration 42, loss = 0.60613696\n",
      "Iteration 43, loss = 0.60716675\n",
      "Iteration 44, loss = 0.60413647\n",
      "Iteration 45, loss = 0.60587278\n",
      "Iteration 46, loss = 0.60833407\n",
      "Iteration 47, loss = 0.61313696\n",
      "Iteration 48, loss = 0.61035658\n",
      "Iteration 49, loss = 0.61174371\n",
      "Iteration 50, loss = 0.60321152\n",
      "Iteration 51, loss = 0.60188433\n",
      "Iteration 52, loss = 0.60521219\n",
      "Iteration 53, loss = 0.60482464\n",
      "Iteration 54, loss = 0.60397660\n",
      "Iteration 55, loss = 0.60501973\n",
      "Iteration 56, loss = 0.59988917\n",
      "Iteration 57, loss = 0.60168277\n",
      "Iteration 58, loss = 0.60130007\n",
      "Iteration 59, loss = 0.59948321\n",
      "Iteration 60, loss = 0.59818906\n",
      "Iteration 61, loss = 0.59982420\n",
      "Iteration 62, loss = 0.59894113\n",
      "Iteration 63, loss = 0.59727882\n",
      "Iteration 64, loss = 0.59704331\n",
      "Iteration 65, loss = 0.59504707\n",
      "Iteration 66, loss = 0.59448593\n",
      "Iteration 67, loss = 0.59619964\n",
      "Iteration 68, loss = 0.59290779\n",
      "Iteration 69, loss = 0.59508847\n",
      "Iteration 70, loss = 0.59509140\n",
      "Iteration 71, loss = 0.59120938\n",
      "Iteration 72, loss = 0.59466011\n",
      "Iteration 73, loss = 0.59747025\n",
      "Iteration 74, loss = 0.59334050\n",
      "Iteration 75, loss = 0.59048557\n",
      "Iteration 76, loss = 0.59348246\n",
      "Iteration 77, loss = 0.59207345\n",
      "Iteration 78, loss = 0.59515150\n",
      "Iteration 79, loss = 0.59130217\n",
      "Iteration 80, loss = 0.58636846\n",
      "Iteration 81, loss = 0.58773010\n",
      "Iteration 82, loss = 0.59123260\n",
      "Iteration 83, loss = 0.58710126\n",
      "Iteration 84, loss = 0.58589004\n",
      "Iteration 85, loss = 0.58329577\n",
      "Iteration 86, loss = 0.58200306\n",
      "Iteration 87, loss = 0.58686281\n",
      "Iteration 88, loss = 0.58569805\n",
      "Iteration 89, loss = 0.58321168\n",
      "Iteration 90, loss = 0.58154300\n",
      "Iteration 91, loss = 0.58385673\n",
      "Iteration 92, loss = 0.57969704\n",
      "Iteration 93, loss = 0.58352985\n",
      "Iteration 94, loss = 0.58027199\n",
      "Iteration 95, loss = 0.57828983\n",
      "Iteration 96, loss = 0.57726384\n",
      "Iteration 97, loss = 0.57587497\n",
      "Iteration 98, loss = 0.57909547\n",
      "Iteration 99, loss = 0.57383058\n",
      "Iteration 100, loss = 0.57852188\n",
      "Iteration 101, loss = 0.57564340\n",
      "Iteration 102, loss = 0.58366476\n",
      "Iteration 103, loss = 0.57817815\n",
      "Iteration 104, loss = 0.57224314\n",
      "Iteration 105, loss = 0.57012461\n",
      "Iteration 106, loss = 0.57422573\n",
      "Iteration 107, loss = 0.57094293\n",
      "Iteration 108, loss = 0.57293594\n",
      "Iteration 109, loss = 0.58322986\n",
      "Iteration 110, loss = 0.57423834\n",
      "Iteration 111, loss = 0.56746542\n",
      "Iteration 112, loss = 0.56529871\n",
      "Iteration 113, loss = 0.57155739\n",
      "Iteration 114, loss = 0.56749502\n",
      "Iteration 115, loss = 0.56758831\n",
      "Iteration 116, loss = 0.56670567\n",
      "Iteration 117, loss = 0.56412005\n",
      "Iteration 118, loss = 0.56230389\n",
      "Iteration 119, loss = 0.56665652\n",
      "Iteration 120, loss = 0.57837913\n",
      "Iteration 121, loss = 0.56221012\n",
      "Iteration 122, loss = 0.56853529\n",
      "Iteration 123, loss = 0.57277834\n",
      "Iteration 124, loss = 0.56936102\n",
      "Iteration 125, loss = 0.56896249\n",
      "Iteration 126, loss = 0.56323547\n",
      "Iteration 127, loss = 0.55626286\n",
      "Iteration 128, loss = 0.56045892\n",
      "Iteration 129, loss = 0.55870011\n",
      "Iteration 130, loss = 0.55657662\n",
      "Iteration 131, loss = 0.55620538\n",
      "Iteration 132, loss = 0.56398991\n",
      "Iteration 133, loss = 0.56547867\n",
      "Iteration 134, loss = 0.55555203\n",
      "Iteration 135, loss = 0.55514814\n",
      "Iteration 136, loss = 0.55276939\n",
      "Iteration 137, loss = 0.55149533\n",
      "Iteration 138, loss = 0.55078425\n",
      "Iteration 139, loss = 0.55332017\n",
      "Iteration 140, loss = 0.54894278\n",
      "Iteration 141, loss = 0.55117336\n",
      "Iteration 142, loss = 0.55127228\n",
      "Iteration 143, loss = 0.55139607\n",
      "Iteration 144, loss = 0.55189639\n",
      "Iteration 145, loss = 0.54740195\n",
      "Iteration 146, loss = 0.54685237\n",
      "Iteration 147, loss = 0.54594557\n",
      "Iteration 148, loss = 0.54663649\n",
      "Iteration 149, loss = 0.55143935\n",
      "Iteration 150, loss = 0.54643293\n",
      "Iteration 151, loss = 0.54429041\n",
      "Iteration 152, loss = 0.54497532\n",
      "Iteration 153, loss = 0.54926516\n",
      "Iteration 154, loss = 0.54654151\n",
      "Iteration 155, loss = 0.54009171\n",
      "Iteration 156, loss = 0.53990853\n",
      "Iteration 157, loss = 0.54331709\n",
      "Iteration 158, loss = 0.54053321\n",
      "Iteration 159, loss = 0.54274557\n",
      "Iteration 160, loss = 0.53916415\n",
      "Iteration 161, loss = 0.54403405\n",
      "Iteration 162, loss = 0.54058422\n",
      "Iteration 163, loss = 0.54126350\n",
      "Iteration 164, loss = 0.53789300\n",
      "Iteration 165, loss = 0.53759039\n",
      "Iteration 166, loss = 0.54078237\n",
      "Iteration 167, loss = 0.53683605\n",
      "Iteration 168, loss = 0.54019799\n",
      "Iteration 169, loss = 0.53166283\n",
      "Iteration 170, loss = 0.53932216\n",
      "Iteration 171, loss = 0.54545530\n",
      "Iteration 172, loss = 0.53927128\n",
      "Iteration 173, loss = 0.53652474\n",
      "Iteration 174, loss = 0.53185986\n",
      "Iteration 175, loss = 0.53328144\n",
      "Iteration 176, loss = 0.53021915\n",
      "Iteration 177, loss = 0.53223921\n",
      "Iteration 178, loss = 0.53054212\n",
      "Iteration 179, loss = 0.52732546\n",
      "Iteration 180, loss = 0.53099671\n",
      "Iteration 181, loss = 0.52744234\n",
      "Iteration 182, loss = 0.53065828\n",
      "Iteration 183, loss = 0.53091399\n",
      "Iteration 184, loss = 0.52902828\n",
      "Iteration 185, loss = 0.52882627\n",
      "Iteration 186, loss = 0.52671296\n",
      "Iteration 187, loss = 0.52526970\n",
      "Iteration 188, loss = 0.53623929\n",
      "Iteration 189, loss = 0.52936993\n",
      "Iteration 190, loss = 0.53228704\n",
      "Iteration 191, loss = 0.53105936\n",
      "Iteration 192, loss = 0.52081432\n",
      "Iteration 193, loss = 0.52259458\n",
      "Iteration 194, loss = 0.52453668\n",
      "Iteration 195, loss = 0.52146956\n",
      "Iteration 196, loss = 0.52240858\n",
      "Iteration 197, loss = 0.52857488\n",
      "Iteration 198, loss = 0.51934713\n",
      "Iteration 199, loss = 0.52610962\n",
      "Iteration 200, loss = 0.52601423\n",
      "Iteration 201, loss = 0.52588530\n",
      "Iteration 202, loss = 0.52294089\n",
      "Iteration 203, loss = 0.52360255\n",
      "Iteration 204, loss = 0.51818895\n",
      "Iteration 205, loss = 0.52635216\n",
      "Iteration 206, loss = 0.52459329\n",
      "Iteration 207, loss = 0.52524356\n",
      "Iteration 208, loss = 0.52707992\n",
      "Iteration 209, loss = 0.52048964\n",
      "Iteration 210, loss = 0.51920341\n",
      "Iteration 211, loss = 0.52137503\n",
      "Iteration 212, loss = 0.52529587\n",
      "Iteration 213, loss = 0.52135009\n",
      "Iteration 214, loss = 0.52485669\n",
      "Iteration 215, loss = 0.51694742\n",
      "Iteration 216, loss = 0.51976553\n",
      "Iteration 217, loss = 0.51887717\n",
      "Iteration 218, loss = 0.52466146\n",
      "Iteration 219, loss = 0.52653036\n",
      "Iteration 220, loss = 0.52333358\n",
      "Iteration 221, loss = 0.52198755\n",
      "Iteration 222, loss = 0.51487537\n",
      "Iteration 223, loss = 0.52075213\n",
      "Iteration 224, loss = 0.52573965\n",
      "Iteration 225, loss = 0.52057443\n",
      "Iteration 226, loss = 0.52252342\n",
      "Iteration 227, loss = 0.51717785\n",
      "Iteration 228, loss = 0.51997986\n",
      "Iteration 229, loss = 0.51153343\n",
      "Iteration 230, loss = 0.51008488\n",
      "Iteration 231, loss = 0.50835445\n",
      "Iteration 232, loss = 0.50956256\n",
      "Iteration 233, loss = 0.51959909\n",
      "Iteration 234, loss = 0.51064029\n",
      "Iteration 235, loss = 0.50979938\n",
      "Iteration 236, loss = 0.51349462\n",
      "Iteration 237, loss = 0.50767050\n",
      "Iteration 238, loss = 0.50931720\n",
      "Iteration 239, loss = 0.52053224\n",
      "Iteration 240, loss = 0.50648183\n",
      "Iteration 241, loss = 0.50526969\n",
      "Iteration 242, loss = 0.50894349\n",
      "Iteration 243, loss = 0.50544478\n",
      "Iteration 244, loss = 0.50574468\n",
      "Iteration 245, loss = 0.50436730\n",
      "Iteration 246, loss = 0.50248480\n",
      "Iteration 247, loss = 0.50102364\n",
      "Iteration 248, loss = 0.50173787\n",
      "Iteration 249, loss = 0.50144877\n",
      "Iteration 250, loss = 0.50047468\n",
      "Iteration 251, loss = 0.50228942\n",
      "Iteration 252, loss = 0.50578210\n",
      "Iteration 253, loss = 0.50060338\n",
      "Iteration 254, loss = 0.49901238\n",
      "Iteration 255, loss = 0.49996472\n",
      "Iteration 256, loss = 0.50101712\n",
      "Iteration 257, loss = 0.49841260\n",
      "Iteration 258, loss = 0.50189643\n",
      "Iteration 259, loss = 0.50008206\n",
      "Iteration 260, loss = 0.49481732\n",
      "Iteration 261, loss = 0.49417846\n",
      "Iteration 262, loss = 0.49667106\n",
      "Iteration 263, loss = 0.49664625\n",
      "Iteration 264, loss = 0.50186402\n",
      "Iteration 265, loss = 0.49336071\n",
      "Iteration 266, loss = 0.49612345\n",
      "Iteration 267, loss = 0.49174881\n",
      "Iteration 268, loss = 0.49347782\n",
      "Iteration 269, loss = 0.49404134\n",
      "Iteration 270, loss = 0.49467035\n",
      "Iteration 271, loss = 0.48952527\n",
      "Iteration 272, loss = 0.49182147\n",
      "Iteration 273, loss = 0.49617941\n",
      "Iteration 274, loss = 0.48525457\n",
      "Iteration 275, loss = 0.49105709\n",
      "Iteration 276, loss = 0.49285742\n",
      "Iteration 277, loss = 0.48582059\n",
      "Iteration 278, loss = 0.48687732\n",
      "Iteration 279, loss = 0.48794251\n",
      "Iteration 280, loss = 0.48636992\n",
      "Iteration 281, loss = 0.48568017\n",
      "Iteration 282, loss = 0.48552164\n",
      "Iteration 283, loss = 0.48342355\n",
      "Iteration 284, loss = 0.48293399\n",
      "Iteration 285, loss = 0.48179481\n",
      "Iteration 286, loss = 0.48531148\n",
      "Iteration 287, loss = 0.48274177\n",
      "Iteration 288, loss = 0.48226132\n",
      "Iteration 289, loss = 0.48091365\n",
      "Iteration 290, loss = 0.47822043\n",
      "Iteration 291, loss = 0.48399563\n",
      "Iteration 292, loss = 0.49412601\n",
      "Iteration 293, loss = 0.48107887\n",
      "Iteration 294, loss = 0.47969967\n",
      "Iteration 295, loss = 0.47704468\n",
      "Iteration 296, loss = 0.47825833\n",
      "Iteration 297, loss = 0.47959429\n",
      "Iteration 298, loss = 0.47476237\n",
      "Iteration 299, loss = 0.47537431\n",
      "Iteration 300, loss = 0.47365862\n",
      "Iteration 301, loss = 0.48103863\n",
      "Iteration 302, loss = 0.47705313\n",
      "Iteration 303, loss = 0.47401389\n",
      "Iteration 304, loss = 0.47452078\n",
      "Iteration 305, loss = 0.47864660\n",
      "Iteration 306, loss = 0.47753837\n",
      "Iteration 307, loss = 0.48906495\n",
      "Iteration 308, loss = 0.48614472\n",
      "Iteration 309, loss = 0.47479964\n",
      "Iteration 310, loss = 0.47509696\n",
      "Iteration 311, loss = 0.48655906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "TP: 761 FN: 239 FP: 207 TN: 793\n",
      "TP: 474 FN: 281 FP: 1508 TN: 3084\n"
     ]
    }
   ],
   "source": [
    "# Neural network model\n",
    "np.random.seed(0)\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 50, 50, 10), max_iter=1000, warm_start=True, solver=\"adam\", activation=\"relu\", learning_rate=\"adaptive\", verbose=True)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print_stats(model, X_train, Y_train)\n",
    "print_stats(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761 0.793\n",
      "0.6278145695364239 0.671602787456446\n"
     ]
    }
   ],
   "source": [
    "print(761/1000, 793/1000) # train accuracy in positive and negative classes\n",
    "print(474/(474+281), 3084/(3084+1508)) # test accuracy in positive and negative classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is still not good: 60-70%<br>\n",
    "We need to reach atleast 85% for reliable stats and worthy paper<br>\n",
    "\n",
    "Let's attempt at LSTMs for sequence<br>\n",
    "We may also need attention models on sequences to focus on certain regions of the protein sequence<br>\n",
    "How about MTL? Using Protein sequence to predict PDB sequence?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM language model Amino Acids of Protein Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 20\n"
     ]
    }
   ],
   "source": [
    "# create mapping of amino acids to integers\n",
    "aa_to_int = {aa: i for i, aa in enumerate(unique_chars.keys())}\n",
    "int_to_aa = {i: aa for aa, i in aa_to_int.items()}\n",
    "\n",
    "print(\"Vocab size =\", len(aa_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should window size be fixed or variable? <br>\n",
    "*** Currently fixing window size to be 20, since minlen is 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4332327, 4332327)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the training dataset of input and output pairs encoded as integers\n",
    "X_train = []\n",
    "Y_train = []\n",
    "chain_len = 20 # should we \n",
    "for protein in pdb_seq_proteins:\n",
    "    seq = pdb_sequences[protein][\"seq\"]\n",
    "    for i in range(0, len(seq)-chain_len):\n",
    "        X_train.append([aa_to_int[aa] for aa in seq[i:i+chain_len]])\n",
    "        Y_train.append(aa_to_int[seq[i+chain_len]])\n",
    "\n",
    "len(X_train), len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MGAMTQLLAGVFLAFLALAT', 'E')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([int_to_aa[i] for i in X_train[0]]), int_to_aa[Y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4332327, 20]) torch.Size([4332327])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.int32)\n",
    "Y_train = torch.tensor(Y_train)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_data = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LSTMTextGeneratorChar(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_len, n_layers, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed_len = embed_len\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_vocab = n_vocab\n",
    "        self.lstm = nn.LSTM(input_size = embed_len, hidden_size = hidden_dim, num_layers = n_layers, device=device, batch_first = True) ### CAREFUL\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(hidden_dim, n_vocab, device=device)\n",
    "\n",
    "        self.word_embedding = nn.Embedding(n_vocab, embed_len)\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        embeddings = self.word_embedding(X_batch)\n",
    "\n",
    "        hidden, carry = torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(device), torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(device)\n",
    "        output, (hidden, carry) = self.lstm(embeddings, (hidden, carry))\n",
    "        return self.linear(self.dropout(output[:,-1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "def train(model, loss_fn, optimizer, train_loader,epochs=10):\n",
    "    set_seed(0)\n",
    "\n",
    "    best_ckpt = None\n",
    "    min_train_loss = 1000000.0\n",
    "    for i in range(1, epochs+1):\n",
    "        losses = []\n",
    "        print(\"Current epoch:\", i)\n",
    "        model.train()\n",
    "\n",
    "        for X_batch, Y_batch in tqdm(train_loader):\n",
    "            Y_pred = model(X_batch.to(device))\n",
    "            loss = loss_fn(Y_pred, Y_batch.to(device))\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(\"Training loss:\", np.mean(losses))\n",
    "        if np.mean(np.array(losses)) < min_train_loss:\n",
    "            min_train_loss = np.mean(losses)\n",
    "            best_ckpt = copy.deepcopy(model)\n",
    "    return best_ckpt, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.58 ms, sys: 11.5 ms, total: 14.1 ms\n",
      "Wall time: 14.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 5e-3\n",
    "embed_len = 6\n",
    "hidden_dim = 128\n",
    "n_layers=2\n",
    "\n",
    "set_seed(42)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "text_generator = LSTMTextGeneratorChar(20, embed_len, n_layers, hidden_dim).to(device)\n",
    "optimizer = Adam(text_generator.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 211/16924 [00:10<13:44, 20.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4522/2537510188.py\", line 1, in <module>\n",
      "    best_checkpoint_char, last_checkpoint_char = train(text_generator, loss_fn, optimizer, train_loader, epochs)\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_4522/164531175.py\", line 13, in train\n",
      "    for X_batch, Y_batch in tqdm(train_loader):\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/tqdm/std.py\", line 1178, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 676, in _next_data\n",
      "    index = self._next_index()  # may raise StopIteration\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 623, in _next_index\n",
      "    return next(self._sampler_iter)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/torch/utils/data/sampler.py\", line -1, in __iter__\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/home/aria/anaconda3/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "best_checkpoint_char, last_checkpoint_char = train(text_generator, loss_fn, optimizer, train_loader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
